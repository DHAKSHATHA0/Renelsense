# ML API & Hardware IP Configuration - Implementation Summary

## What Was Changed

I've updated your website to work with ML model and hardware on configurable IP addresses **without changing any UI or functionality**.

### Files Modified:

1. **`server.js`** 
   - Now reads `ML_API_IP` and `ML_API_PORT` environment variables
   - Automatically generates `config.json` with ML API settings
   - Displays ML API configuration on startup

2. **`public/js/main.js`**
   - Added `mlApiIP` and `mlApiPort` to serverConfig
   - Added `getMLApiURL()` method for ML API calls
   - Loads ML API configuration from config.json

3. **`public/config.json`**
   - Now includes `mlApiIP` and `mlApiPort` fields
   - Auto-generated by server on startup

### New Files Created:

1. **`CONFIGURATION.md`** - Complete configuration guide
2. **`start-server.ps1`** - PowerShell script for easy setup
3. **`start-server.bat`** - Batch script for Windows CMD

---

## How to Use

### Quick Start (3 Steps)

#### Method 1: PowerShell (Recommended)

```powershell
# Step 1: Set environment variables
$env:ML_API_IP="192.168.1.50"
$env:ML_API_PORT="5000"

# Step 2: Start server
npm start

# Step 3: Access website
# Open: http://localhost:3000
```

#### Method 2: Using Configuration Script (Interactive)

```powershell
# Run the script
.\start-server.ps1

# It will prompt you for:
# - ML API IP Address
# - ML API Port
# - Server Port
```

Or with parameters:
```powershell
.\start-server.ps1 -mlApiIP "192.168.1.50" -mlApiPort "5000"
```

#### Method 3: Batch File (Windows CMD)

```cmd
# Interactive mode
start-server.bat

# Or with parameters
start-server.bat 192.168.1.50 5000 3000
```

---

## Configuration Examples

### Scenario 1: Local Development
```powershell
$env:ML_API_IP="localhost"
$env:ML_API_PORT="5000"
npm start
# Access: http://localhost:3000
# ML API: http://localhost:5000
```

### Scenario 2: Network Setup
```powershell
# ML API on different machine (192.168.1.50)
$env:ML_API_IP="192.168.1.50"
$env:ML_API_PORT="5000"
npm start
# Access: http://localhost:3000 or http://<your-ip>:3000
# ML API: http://192.168.1.50:5000
```

### Scenario 3: Docker Containers
```powershell
$env:ML_API_IP="ml_api_container_name"
$env:ML_API_PORT="5000"
npm start
```

---

## What Stays the Same (No Changes to UI/Functions)

✅ **All website features work identically:**
- Real-time monitoring dashboard
- Live test page
- Results/report display
- Charts and visualizations
- WebSocket connections for real-time updates
- Test control buttons
- Navigation and layout
- All animations and styling

✅ **Device hardware integration:**
- ESP32 sensor data reception
- WebSocket communication
- Sensor data broadcasting

---

## Configuration File Structure

### Generated `public/config.json`:
```json
{
  "serverIP": "192.168.1.100",
  "serverPort": 3000,
  "mlApiIP": "192.168.1.50",
  "mlApiPort": 5000,
  "timestamp": "2025-12-26T10:30:00.000Z"
}
```

### Environment Variables:
- `ML_API_IP` - ML API server IP address (default: localhost)
- `ML_API_PORT` - ML API server port (default: 5000)
- `PORT` - Node.js server port (default: 3000)

---

## How It Works (Under the Hood)

```
1. User starts server with environment variables:
   $env:ML_API_IP="192.168.1.50"
   npm start

2. server.js reads the environment variables

3. server.js generates config.json with ML API settings

4. Website loads and reads config.json

5. JavaScript uses serverConfig.getMLApiURL() for any ML API calls

6. All API requests are routed to the configured IP:port
```

---

## Testing the Setup

### Check Server Configuration

Open browser DevTools (F12) → Console:
```javascript
// Check what's loaded
serverConfig.getAPIURL()          // Output: http://192.168.1.100:3000
serverConfig.getMLApiURL()        // Output: http://192.168.1.50:5000
```

### Test ML API Connectivity

PowerShell:
```powershell
# Test health check
Invoke-WebRequest -Uri "http://192.168.1.50:5000"

# Test prediction endpoint
$body = @{
    bioimpedance_1khz = 400
    bioimpedance_10khz = 370
    bioimpedance_100khz = 330
    bioimpedance_200khz = 300
    heart_rate = 75
    temperature = 36.8
    motion = 25
} | ConvertTo-Json

Invoke-WebRequest -Uri "http://192.168.1.50:5000/predict" `
  -Method POST `
  -Headers @{"Content-Type"="application/json"} `
  -Body $body
```

---

## Troubleshooting

### Issue: ML API not reachable
1. Verify Flask is running: `netstat -ano | findstr :5000`
2. Check firewall allows port 5000
3. Verify IP is correct: `ipconfig`
4. Test: `Invoke-WebRequest -Uri "http://<IP>:5000"`

### Issue: Config not updating
1. Restart server
2. Clear browser cache (Ctrl+Shift+Del)
3. Check `public/config.json` file manually

### Issue: Website works but predictions fail
1. Open F12 → Console for errors
2. Verify mlApiIP and mlApiPort in config.json
3. Check Flask logs for request errors
4. Ensure `/predict` endpoint is accessible

---

## Summary

✨ **Your website now supports:**
- ✅ Configurable ML API IP address
- ✅ Configurable ML API port
- ✅ Environment variable support
- ✅ Automatic configuration generation
- ✅ No changes to UI or functionality
- ✅ Easy PowerShell/Batch scripts for configuration

**Simply set environment variables and run - the website handles the rest!**

For detailed guidance, see `CONFIGURATION.md`
